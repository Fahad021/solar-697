{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e3686d-4829-4253-b39f-dfd5d6a03066",
   "metadata": {},
   "source": [
    "### NSRDB Aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f75a151-807b-4ccc-a7f1-9f735317e667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import site\n",
    "import sqlite3\n",
    "import sys\n",
    "\n",
    "import logzero\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from logzero import logger\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from yaml import dump, load, safe_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03ebae6f-c7d2-4aab-9b92-2ba92352e55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../source\")\n",
    "import queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df7ea666-17e0-4f2e-8c27-85f599650937",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = \"../logs/\"\n",
    "log_file = \"nsrdb_aggregator.log\"\n",
    "\n",
    "logzero.logfile(log_path + log_file, maxBytes=1e6, backupCount=5, disableStderrLogger=True)\n",
    "logger.info(f\"{log_path}, {log_file}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54798d9e-ad6f-48a0-ae8f-fb98a9ecd9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = None\n",
    "try:\n",
    "    with open(\"../config.yml\", \"r\") as config_in:\n",
    "        configs = load(config_in, Loader=yaml.SafeLoader)\n",
    "        logger.info(f\"{configs}\\n\")\n",
    "except:\n",
    "    logger.error(f\"config file open failure.\")\n",
    "    exit(1)\n",
    "\n",
    "cfg_vars = configs[\"url_variables\"]\n",
    "logger.info(f\"variables: {cfg_vars}\\n\")\n",
    "\n",
    "years = configs[\"request_years\"]\n",
    "logger.info(f\"years: {years}\\n\")\n",
    "\n",
    "db_path = configs[\"file_paths\"][\"download_db_path\"]\n",
    "data_path = configs[\"file_paths\"][\"download_path_zips\"]\n",
    "csv_path = configs[\"file_paths\"][\"download_path_raw\"]\n",
    "\n",
    "city = configs[\"location_info\"][\"city\"]\n",
    "state = configs[\"location_info\"][\"state\"]\n",
    "db_file = city + \"_\" + state + \".db\"\n",
    "\n",
    "# db_file = \"nsrdb_daily.db\"\n",
    "db_file = \"nsrdb_monthly_f.db\"\n",
    "# period = \"D\"\n",
    "period = \"M\"\n",
    "\n",
    "# db_file = \"geo_zipcodes.db\"\n",
    "\n",
    "db_table1 = configs[\"table_names\"][\"db_table1\"]\n",
    "db_table2 = configs[\"table_names\"][\"db_table2\"]\n",
    "\n",
    "db_file2 = configs[\"file_names\"][\"db_file_gzc\"]\n",
    "\n",
    "logger.info(f\"{db_path}, {db_file}\")\n",
    "\n",
    "nrows = configs[\"num_rows\"][0]\n",
    "zip_import = configs[\"zip_import\"][True]\n",
    "\n",
    "logger.info(f\"number of rows: {nrows}\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc3fc360-f7eb-4f3a-acbe-db5f950168a0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "try:\n",
    "    with open(data_path + \"zipcodes_\" + city + \"_\" + state + \".yml\", \"r\") as locs_in:\n",
    "        locs = load(locs_in, Loader=yaml.SafeLoader)\n",
    "        logger.info(locs)\n",
    "except:\n",
    "    logger.error(f\"location file open failure.\")\n",
    "    exit(1)\n",
    "\n",
    "zip_codes = locs[\"zipcodes\"]\n",
    "\n",
    "logger.info(f\"zip codes: {zip_codes}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dd7a919-d08c-437c-ad7c-b29fbaac7453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../../data/db/ nsrdb_monthly_f.db geo_zipcodes.db\n",
      "3197 3197\n",
      "['nsrdb_12019_1998.csv', 'nsrdb_12019_1999.csv', 'nsrdb_12019_2000.csv']\n",
      "['nsrdb_meta_12019_1998.csv', 'nsrdb_meta_12019_1999.csv', 'nsrdb_meta_12019_2000.csv']\n"
     ]
    }
   ],
   "source": [
    "# csv_path = \"/home/gmyers/_UMSI/697/data/nsrdb_csv2/\"\n",
    "\n",
    "nsrdb_conv = \"nsrdb_?????_????.csv\"\n",
    "nsrdb_meta_conv = \"nsrdb_meta_?????_????.csv\"\n",
    "\n",
    "\n",
    "def get_csv_files(csv_path, name_conv):\n",
    "    files = [file.split(\"/\")[-1] for file in glob.glob(csv_path + name_conv)]\n",
    "    return sorted(files)\n",
    "\n",
    "\n",
    "nsrdb_files = get_csv_files(csv_path, nsrdb_conv)\n",
    "nsrdb_meta_files = get_csv_files(csv_path, nsrdb_meta_conv)\n",
    "print(db_path, db_file, db_file2)\n",
    "print(len(nsrdb_files), len(nsrdb_meta_files))\n",
    "print(nsrdb_files[:3])\n",
    "print(nsrdb_meta_files[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a48fdadf-0cab-493d-92df-d802a46185fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nsrdb_monthly_f.db'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_path\n",
    "db_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0c0856c-1a82-44d0-9212-b85c00bcfade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish db connection and cursor\n",
    "conn = sqlite3.connect(db_path + db_file)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39ecb794-ec4f-42d7-a321-f39a11faf359",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(queries.create_table_monthly_nsrdb)\n",
    "conn.commit()\n",
    "\n",
    "cursor.execute(queries.create_table_geo_zipcodes)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca3a0fde-af4f-4384-beea-55d7b79c8404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\"path\": db_path, \"db_file2\": db_file2}\n",
    "# need to test for existance of records in the db\n",
    "# and skip the import if so\n",
    "if zip_import:\n",
    "    cursor.execute(\"\"\"ATTACH DATABASE '../../../data/db/geo_zipcodes.db' AS gzc_db;\"\"\")\n",
    "    cursor.execute(\"\"\"INSERT INTO 'geo_zipcodes' SELECT * FROM gzc_db.geo_zipcodes;\"\"\")\n",
    "    conn.commit()\n",
    "    cursor.execute(\"DETACH gzc_db\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3084851a-b3cd-4196-bc33-d0090746150f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "add_ccs = [\n",
    "    \"ALTER TABLE geo_zipcodes ADD city TEXT;\",\n",
    "    \"ALTER TABLE geo_zipcodes ADD county TEXT;\",\n",
    "    \"ALTER TABLE geo_zipcodes ADD state TEXT;\",\n",
    "]\n",
    "\n",
    "for qry in add_ccs:\n",
    "    cursor.execute(qry)\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "990d011e-3552-42c0-8fdf-543073e39eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zip = pd.read_csv(\"../../../../data/zip_code_database.csv\")\n",
    "df_zip.set_index(\"zip\", inplace=True, verify_integrity=True)\n",
    "# df_zip.loc[74145][[\"primary_city\", \"state\", \"county\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d44e981-225f-4eb5-8870-95c00881e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry_zips = \"select DISTINCT zipcode from geo_zipcodes;\"\n",
    "zips_qry = cursor.execute(qry_zips)\n",
    "zipss = [x[0] for x in list(zips_qry)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b87906ae-5226-46a9-bc00-18a65ac05559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, zips in enumerate(zipss):\n",
    "for zips in zipss:  # tqdm(zipss):\n",
    "    locale_data = df_zip.loc[int(zips)][[\"primary_city\", \"state\", \"county\"]].to_dict()\n",
    "    locale_data.update({\"zipcode\": zips})\n",
    "\n",
    "    cursor.execute(\n",
    "        f\"\"\"update geo_zipcodes SET city=:primary_city,\n",
    "        state=:state, county=:county\n",
    "        where zipcode=:zipcode;\"\"\",\n",
    "        locale_data,\n",
    "    )\n",
    "\n",
    "conn.commit()\n",
    "#     if i == 5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49d88db-749d-4a53-9bb4-8fe3cd0de857",
   "metadata": {},
   "source": [
    "### Download link information\n",
    "https://developer.nrel.gov/docs/solar/nsrdb/psm3-download/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f0c192b-a931-4e3a-8036-e015e9b7adba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"date_time\",\n",
    "    \"zipcode\",\n",
    "    \"location_id\",\n",
    "    \"Year\",\n",
    "    \"Month\",\n",
    "    \"Day\",\n",
    "    \"Hour\",\n",
    "    \"Temperature\",\n",
    "    \"Clearsky_DHI\",\n",
    "    \"Clearsky_DNI\",\n",
    "    \"Clearsky_GHI\",\n",
    "    \"Cloud_Type\",\n",
    "    \"Dew_Point\",\n",
    "    \"DHI\",\n",
    "    \"DNI\",\n",
    "    \"Fill_Flag\",\n",
    "    \"GHI\",\n",
    "    \"Relative_Humidity\",\n",
    "    \"Solar_Zenith_Angle\",\n",
    "    \"Surface_Albedo\",\n",
    "    \"Pressure\",\n",
    "    \"Precipitable_Water\",\n",
    "    \"Wind_Direction\",\n",
    "    \"Wind_Speed\",\n",
    "    \"Global_Horizontal_UV_Irradiance_(280-400nm)\",\n",
    "    \"Global_Horizontal_UV_Irradiance_(295-385nm)\",\n",
    "]\n",
    "cols = [\n",
    "    \"date_time\",\n",
    "    \"zipcode\",\n",
    "    \"location_id\",\n",
    "    \"Temperature\",\n",
    "    \"Clearsky_DHI\",\n",
    "    \"Clearsky_DNI\",\n",
    "    \"Clearsky_GHI\",\n",
    "    \"Dew_Point\",\n",
    "    \"DHI\",\n",
    "    \"DNI\",\n",
    "    \"GHI\",\n",
    "    \"Relative_Humidity\",\n",
    "    \"Pressure\",\n",
    "    \"Precipitable_Water\",\n",
    "    \"Wind_Speed\",\n",
    "    \"Global_Horizontal_UV_Irradiance_(280-400nm)\",\n",
    "    \"Global_Horizontal_UV_Irradiance_(295-385nm)\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6620ae97-2837-4597-b025-6ad30050e92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3196\r"
     ]
    }
   ],
   "source": [
    "debug = False\n",
    "# for idx, file in enumerate(nsrdb_files):\n",
    "for i, file in enumerate(nsrdb_files):  # tqdm(nsrdb_files):\n",
    "    print(i, end=\"\\r\")\n",
    "    df = pd.read_csv(csv_path + file, parse_dates=True, usecols=cols)\n",
    "    df[\"date_time\"] = pd.to_datetime(df[\"date_time\"])\n",
    "\n",
    "    df = df.set_index(\"date_time\").resample(period).mean()\n",
    "    df = df.round(decimals=5).reset_index(drop=False, inplace=False)\n",
    "    df.rename(\n",
    "        {\n",
    "            \"Global_Horizontal_UV_Irradiance_(280-400nm)\": \"GHI_UV_wd\",\n",
    "            \"Global_Horizontal_UV_Irradiance_(295-385nm)\": \"GHI_UV_nw\",\n",
    "        },\n",
    "        axis=1,\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    df[\"zipcode\"] = df[\"zipcode\"].astype(\"int64\")\n",
    "    df[\"zipcode\"] = df[\"zipcode\"].astype(\"object\")\n",
    "    df[\"location_id\"] = df[\"location_id\"].astype(\"int64\")\n",
    "\n",
    "    if debug:\n",
    "        display(df)\n",
    "\n",
    "    df.to_sql(\"nsrdb\", conn, if_exists=\"append\", index=False, method=\"multi\")\n",
    "\n",
    "#     if idx == 2:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9f739ff-23fb-4b82-baaf-98e942341122",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62ac13f3-3d18-42ed-bb63-3d7794ee6027",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "for year in years:\n",
    "    for zip_code in tqdm(zip_codes.keys()):\n",
    "\n",
    "        # query and extract the first 2 lines to get metadata:\n",
    "        df_meta = df_raw.iloc[0].copy()\n",
    "        # display(df_meta)\n",
    "\n",
    "        row1_cols = df_raw.iloc[1]\n",
    "        new_header = [item.replace(\" \", \"_\") if isinstance(item, str) else item for item in row1_cols]\n",
    "\n",
    "        df_data = df_raw.iloc[1:].copy()\n",
    "        df_data.columns = new_header\n",
    "        df_data.drop(1, axis=0, inplace=True)\n",
    "        df_data = df_data.loc[:, df_data.columns.notnull()].copy()\n",
    "        df_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        df_data.insert(0, \"date_time\", \"\")\n",
    "\n",
    "        df_data[\"date_time\"] = pd.to_datetime(\n",
    "            df_data[\"Year\"].astype(str)\n",
    "            + \"-\"\n",
    "            + df_data[\"Month\"].astype(str)\n",
    "            + \"-\"\n",
    "            + df_data[\"Day\"].astype(str)\n",
    "            + \" \"\n",
    "            + df_data[\"Hour\"].astype(str)\n",
    "            + \":\"\n",
    "            + df_data[\"Minute\"].astype(str)\n",
    "            + \":\"\n",
    "            + \"00\"\n",
    "        )\n",
    "\n",
    "        # df_data.drop([\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"], axis=1, inplace=True)\n",
    "        df_data.drop([\"Minute\"], axis=1, inplace=True)\n",
    "        df_data.insert(1, \"zipcode\", zip_code)\n",
    "        df_data.insert(2, \"location_id\", df_meta[\"Location ID\"])\n",
    "\n",
    "        data_names = [\n",
    "            (df_data, \"nsrdb_\" + str(zip_code) + \"_\" + str(year) + \".csv\"),\n",
    "            (df_meta, \"nsrdb_meta_\" + str(zip_code) + \"_\" + str(year) + \".csv\"),\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            for item in data_names:\n",
    "                item[0].to_csv(\"~/_UMSI/697/data/nsrdb_raw/\" + item[1], index=True)\n",
    "                logger.info(f\"{item[1]} successfully written.\\n\")\n",
    "        except:\n",
    "            logger.error(\"Error writing .csv raw file(s)\")\n",
    "\n",
    "        try:\n",
    "            cursor.execute(queries.select_zip_year, {\"zipcode\": zip_code, \"year\": year})\n",
    "            count = cursor.fetchone()\n",
    "            # print(count)\n",
    "\n",
    "            if (count[0] == \"8760\") or (count[0] == \"8784\"):\n",
    "                logger.warning(f\"data for {year}, {zip_code} already present\\n\")\n",
    "            else:\n",
    "                df_data.to_sql(\"nsrdb\", conn, if_exists=\"append\", index=False, method=\"multi\")\n",
    "                logger.info(f\"data for {year}, {zip_code} written to {db_file}:{db_table1}\\n\")\n",
    "        except:\n",
    "            logger.error(\"Error writing to nsrdb\\n\")\n",
    "\n",
    "        llltze_params = {\n",
    "            \"loc_id\": df_meta[\"Location ID\"],\n",
    "            \"lat\": df_meta[\"Latitude\"],\n",
    "            \"lon\": df_meta[\"Longitude\"],\n",
    "            \"elev\": df_meta[\"Elevation\"],\n",
    "            \"tz\": df_meta[\"Time Zone\"],\n",
    "            \"zipcode\": zip_code,\n",
    "        }\n",
    "        logger.info(f\"{llltze_params}\\n\")\n",
    "\n",
    "        cursor.execute(queries.update_gzc_llltze, llltze_params)\n",
    "        conn.commit()\n",
    "\n",
    "        cursor2.execute(queries.update_gzc_llltze, llltze_params)\n",
    "        conn2.commit()\n",
    "\n",
    "        cursor.execute(queries.select_zipcode, {\"zipcode\": zip_code})\n",
    "        logger.info(f\"gzc: {cursor.fetchall()}\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a82f7add-8b46-4054-94e0-56d1def813b7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "for year in years:\n",
    "    for zip_code in tqdm(zip_codes.keys()):\n",
    "        req_str = (\n",
    "            f\"https://developer.nrel.gov/api/solar/nsrdb_psm3_download.csv?\"\n",
    "            + f'wkt=POINT({zip_codes[zip_code][\"lon\"]}%20{zip_codes[zip_code][\"lat\"]})'\n",
    "            + f\"&names={year}\"\n",
    "            + f'&leap_day={cfg_vars[\"leap_year\"]}'\n",
    "            + f'&interval={cfg_vars[\"interval\"]}'\n",
    "            + f'&utc={cfg_vars[\"utc\"]}'\n",
    "            + f'&full_name={cfg_vars[\"name\"]}'\n",
    "            + f'&email={cfg_vars[\"email\"]}'\n",
    "            + f'&affiliation={cfg_vars[\"affiliation\"]}'\n",
    "            + f'&mailing_list={cfg_vars[\"mailing_list\"]}'\n",
    "            + f'&reason={cfg_vars[\"use\"]}'\n",
    "            + f'&api_key={cfg_vars[\"key\"]}'\n",
    "            + f'&attributes={cfg_vars[\"attrs\"]}'\n",
    "        )\n",
    "\n",
    "        logger.info(f\"{req_str}\\n\")\n",
    "\n",
    "        # sleep so we don't overrun the rate NREL limit\n",
    "        sleep(2)\n",
    "        try:\n",
    "            df_raw = pd.read_csv(req_str, nrows=nrows)\n",
    "            logger.info(\"reg_str successful.\")\n",
    "        except:\n",
    "            logger.error(f\"Error requesting\\n{req_str}\\n\")\n",
    "\n",
    "        # query and extract the first 2 lines to get metadata:\n",
    "        df_meta = df_raw.iloc[0].copy()\n",
    "        # display(df_meta)\n",
    "\n",
    "        row1_cols = df_raw.iloc[1]\n",
    "        new_header = [item.replace(\" \", \"_\") if isinstance(item, str) else item for item in row1_cols]\n",
    "\n",
    "        df_data = df_raw.iloc[1:].copy()\n",
    "        df_data.columns = new_header\n",
    "        df_data.drop(1, axis=0, inplace=True)\n",
    "        df_data = df_data.loc[:, df_data.columns.notnull()].copy()\n",
    "        df_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        df_data.insert(0, \"date_time\", \"\")\n",
    "\n",
    "        df_data[\"date_time\"] = pd.to_datetime(\n",
    "            df_data[\"Year\"].astype(str)\n",
    "            + \"-\"\n",
    "            + df_data[\"Month\"].astype(str)\n",
    "            + \"-\"\n",
    "            + df_data[\"Day\"].astype(str)\n",
    "            + \" \"\n",
    "            + df_data[\"Hour\"].astype(str)\n",
    "            + \":\"\n",
    "            + df_data[\"Minute\"].astype(str)\n",
    "            + \":\"\n",
    "            + \"00\"\n",
    "        )\n",
    "\n",
    "        # df_data.drop([\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"], axis=1, inplace=True)\n",
    "        df_data.drop([\"Minute\"], axis=1, inplace=True)\n",
    "        df_data.insert(1, \"zipcode\", zip_code)\n",
    "        df_data.insert(2, \"location_id\", df_meta[\"Location ID\"])\n",
    "\n",
    "        data_names = [\n",
    "            (df_data, \"nsrdb_\" + str(zip_code) + \"_\" + str(year) + \".csv\"),\n",
    "            (df_meta, \"nsrdb_meta_\" + str(zip_code) + \"_\" + str(year) + \".csv\"),\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            for item in data_names:\n",
    "                item[0].to_csv(\"~/_UMSI/697/data/nsrdb_raw/\" + item[1], index=True)\n",
    "                logger.info(f\"{item[1]} successfully written.\\n\")\n",
    "        except:\n",
    "            logger.error(\"Error writing .csv raw file(s)\")\n",
    "\n",
    "        try:\n",
    "            cursor.execute(queries.select_zip_year, {\"zipcode\": zip_code, \"year\": year})\n",
    "            count = cursor.fetchone()\n",
    "            # print(count)\n",
    "\n",
    "            if (count[0] == \"8760\") or (count[0] == \"8784\"):\n",
    "                logger.warning(f\"data for {year}, {zip_code} already present\\n\")\n",
    "            else:\n",
    "                df_data.to_sql(\"nsrdb\", conn, if_exists=\"append\", index=False, method=\"multi\")\n",
    "                logger.info(f\"data for {year}, {zip_code} written to {db_file}:{db_table1}\\n\")\n",
    "        except:\n",
    "            logger.error(\"Error writing to nsrdb\\n\")\n",
    "\n",
    "        llltze_params = {\n",
    "            \"loc_id\": df_meta[\"Location ID\"],\n",
    "            \"lat\": df_meta[\"Latitude\"],\n",
    "            \"lon\": df_meta[\"Longitude\"],\n",
    "            \"elev\": df_meta[\"Elevation\"],\n",
    "            \"tz\": df_meta[\"Time Zone\"],\n",
    "            \"zipcode\": zip_code,\n",
    "        }\n",
    "        logger.info(f\"{llltze_params}\\n\")\n",
    "\n",
    "        cursor.execute(queries.update_gzc_llltze, llltze_params)\n",
    "        conn.commit()\n",
    "\n",
    "        cursor2.execute(queries.update_gzc_llltze, llltze_params)\n",
    "        conn2.commit()\n",
    "\n",
    "        cursor.execute(queries.select_zipcode, {\"zipcode\": zip_code})\n",
    "        logger.info(f\"gzc: {cursor.fetchall()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c487d69-9a8f-46d9-894b-87fd0b6237ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
